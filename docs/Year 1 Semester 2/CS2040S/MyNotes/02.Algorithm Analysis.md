# Algorithm Analysis

>[!abstract]- Resources
> [CS2040_03.Searching](../Notes/CS2040_03.Searching.pdf) - [Video](https://www.youtube.com/watch?v=oblot5d-ATI&t=2026s)

## Motivation

In theoretical analysis of algorithms, it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input.

Analysis of algorithm is the process of analyzing the problem-solving capability of the algorithm in terms of the time and size required (the size of memory for storage while implementation). However, the main concern of analysis of algorithms is the required time or performance. Generally, we perform the following types of analysis 

-   **Worst-case** − The maximum number of steps taken on any instance of size **a**.
-   **Best-case** − The minimum number of steps taken on any instance of size **a**.
-   **Average case** − An average number of steps taken on any instance of size **a**.

## Notations

| Big Oh (O)                                                                | Big Omega (Ω)                                                 | Big Theta (Θ)                                                     |
| ------------------------------------------------------------------------- | ------------------------------------------------------------- | ----------------------------------------------------------------- |
| Rate of growth of an algorithm is less than or equal to a specific value. | Rate of growth is greater than or equal to a specified value. | Rate of growth is equal to a specified value.                     |
| Upper Bound                                                               | Lower Bound                                                   | Tight Bound                                                       |
| $T(\mathrm{n})=\mathrm{O}(f(n))$ if $T$ grows no faster than $f$          | $T(n)=\Omega(f(n))$ if $T$ grows no slower than $\mathrm{f}$  | $T(n)=\Theta(f(n))$ if $T$ grows at the same rate as $\mathrm{f}$ |
| $T(n) \leq c f(n)$                                                        | $T(n) \geq c f(n)$                                            | $T(n)=\mathrm{O}(f(n)) \text { and }$ $T(n)=\Omega(f(n))$                                                                  |

## General Rules

| Notation                                         | Example                                                                                                                                                                    |
| ------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| If $T(n)$ is a polynomial of degree $k$ then:<br>$T(n)=\mathrm{O}\left(n^k\right)$                | $10 n^5+50 n^3+10 n+17=\mathrm{O}\left(n^5\right)$                                                                                                                         |
| $T(n)+S(n)=O(f(n)+g(n))$                         | $10 n^2=\mathrm{O}\left(n^2\right)$<br>$5 n \log (n)=\mathrm{O}(n \log (n))$<br>$10 n^2+5 n \log (n)=\mathrm{O}\left(n^2+n \log (n)\right)=\mathrm{O}\left(n^2\right)$<br> |
| $T(n) \times S(n)=\mathrm{O}\left(f(n) \times g(n)\right)$ | $10 n^2=\mathrm{O}\left(n^2\right)$<br>$5 n=\mathrm{O}(n)$<br>$\left(10 n^2\right)(5 n)=50 n^3=\mathrm{O}\left(n^* n^2\right)=\mathrm{O}\left(n^3\right)$                                                                                                                                                                           |

Difficult Example: $\log (n !)= O(n \log n)$

## Algorithm Analysis

> [!info]- Model of computation
> In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized.
> 
> In this course, we will be focusing on Sequential Computer where all operations and executed one at a time, taking a constant time.

Here are the **general rules** for analyzing the time complexity of a given code:

- **Loops:** `cost = (# iterations)x(max cost of one iteration)`
- **Nested Loops:** `cost = (# iterations)(max cost of one iteration)`
- **Sequential statements:** `cost = (cost of first) + (cost of second)`
- **if / else statements:** `cost = max(cost of first, cost of second)` or in general just add the two: `(cost of first) + (cost of second)`

### Recurrences

For recursive cals, it is best to understand how to calculate the time complexity with examples:

$$
T(n) = 1+T((n-1)+T(n-2))
$$
```java
int fib(int n) {
	if(n <= 1)
		return n;
	else
		return fib(n-1) + fib(n-2); // T(n-1) + T(n-1)
}
```

We will not be going to the mathematics of calculating such formulas, but it is best to calculate such equations using a tree. Refer to [CS2040_tut01](../Tutorial/tut1/CS2040S_tut01.pdf) for asymptotic analysis questions.

## Precondition and Postcondition

**Precondition**:

- Fact that is true when the function begins.
- Usually important for it to work correctly.

**Postcondition**:

- Fact that is true when the function ends.
- Usually useful to show that the computation was done correctly.

## Invariants

**Invariant**:

- relationship between variables that is always true.

**Loop Invariant**:

- relationship between variables that is true at the beginning (or end) of each iteration of a loop.